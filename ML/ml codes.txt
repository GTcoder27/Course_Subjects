practical 1
1. # Importing Python libraries commonly used in Data Science
2.
3. import numpy as np                # For numerical computations
4. import pandas as pd               # For data manipulation and
analysis
5. import matplotlib.pyplot as plt   # For basic data visualization
6. import seaborn as sns             # For advanced and beautiful
visualizations
7. %matplotlib inline
8.
9. print(&quot;Libraries imported successfully!&quot;)
10.
11. # Load dataset directly from seaborn&#39;s built-in datasets
12. df = sns.load_dataset(&#39;titanic&#39;)
13.
14. # Display first 5 rows
15. df.head()
1. # Display basic info about dataset
2. df.info()
3.
4. # Basic statistical summary of numerical columns
5. df.describe()
6.
7. # Checking for missing values
8. df.isnull().sum()
1. # Drop columns that are not useful for analysis
2. df = df.drop([&#39;deck&#39;, &#39;embark_town&#39;], axis=1)
3.
4. # Fill missing age values with median
5. df[&#39;age&#39;].fillna(df[&#39;age&#39;].median(), inplace=True)
6.
7. # Fill missing &#39;embarked&#39; values with the mode (most frequent
value)
8. df[&#39;embarked&#39;].fillna(df[&#39;embarked&#39;].mode()[0], inplace=True)
9.
10. # Convert &#39;sex&#39; to numeric values (male=0, female=1)
11. df[&#39;sex&#39;] = df[&#39;sex&#39;].map({&#39;male&#39;: 0, &#39;female&#39;: 1})
12.
13. # Verify cleaning
14. df.isnull().sum()
a. plt.figure(figsize=(8,5))
b. sns.boxplot(x=&#39;pclass&#39;, y=&#39;age&#39;, data=df, palette=&#39;Set2&#39;)
c. plt.title(&quot;Passenger Class vs Age&quot;)
d. plt.show()
____________________________________________
practical 2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Scikit-learn modules for dataset, model, and metrics
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
)
# Load the dataset
iris = load_iris()
X = iris.data
y = iris.target
# Convert to DataFrame for better understanding
df = pd.DataFrame(X, columns=iris.feature_names)
df[&#39;target&#39;] = iris.target
print(&quot;Iris Dataset Loaded Successfully!&quot;)
print(df.head())
# 80% for training, 20% for testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f&quot;Training samples: {len(X_train)}, Testing samples: {len(X_test)}&quot;)
# Initialize the Gaussian Naive Bayes model
model = GaussianNB()
# Fit the model on training data
model.fit(X_train, y_train)
# Predict on test data
y_pred = model.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f&quot; Model Accuracy: {accuracy*100:.2f}%&quot;)
# Generate classification report
print(&quot;\nClassification Report:&quot;)
print(classification_report(y_test, y_pred, target_names=iris.target_names))
# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(&quot;\n Confusion Matrix:\n&quot;, cm)
____________________________________________
practical 3
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Load dataset
data = load_boston()
X = data.data[:, [5]]  # Using only "RM" feature (avg rooms per house)
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

train_errors = []
test_errors = []
degrees = range(1, 10)

for d in degrees:
    # Polynomial transform
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    # Train model
    model = LinearRegression()
    model.fit(X_train_poly, y_train)

    # Predictions
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)

    # Mean Squared Errors
    train_errors.append(mean_squared_error(y_train, y_train_pred))
    test_errors.append(mean_squared_error(y_test, y_test_pred))

# Plot Bias-Variance Tradeoff
plt.figure(figsize=(7,4))
plt.plot(degrees, train_errors, marker='o', label="Train Error")
plt.plot(degrees, test_errors, marker='s', label="Test Error")
plt.xlabel("Polynomial Degree")
plt.ylabel("MSE")
plt.title("Bias-Variance Tradeoff")
plt.legend()
plt.grid(True)
plt.show()
____________________________________________
practical 4
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Load Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split into train & test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create SVM classifier (RBF kernel)
model = SVC(kernel='rbf', C=1.0, gamma='scale')

# Train model
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
____________________________________________
practical 5
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load California Housing dataset
data = fetch_california_housing()
X = data.data
y = data.target

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("R² Score:", r2)
____________________________________________
practical 6
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load dataset
iris = load_iris()
X = iris.data

# ---------------------------
# 1. Apply K-Means Clustering
# ---------------------------
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# ---------------------------
# 2. Apply PCA (2 Components)
# ---------------------------
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# ---------------------------
# 3. Visualize Clusters
# ---------------------------
plt.figure(figsize=(6, 5))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=60)
plt.title("K-Means Clustering (Visualized with PCA)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.grid(True)
plt.show()
____________________________________________
practical 7
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training & testing data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create Decision Tree classifier
model = DecisionTreeClassifier(criterion="gini", max_depth=None, random_state=42)

# Train model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
____________________________________________
practical 8
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Bagging with Decision Trees
bagging = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=20,
    random_state=42
)

# Train
bagging.fit(X_train, y_train)

# Predict
y_pred = bagging.predict(X_test)

print("Bagging Accuracy:", accuracy_score(y_test, y_pred))
____________________________________________
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

# AdaBoost with Decision Trees
boost = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)

# Train
boost.fit(X_train, y_train)

# Predict
y_pred2 = boost.predict(X_test)

print("Boosting (AdaBoost) Accuracy:", accuracy_score(y_test, y_pred2))