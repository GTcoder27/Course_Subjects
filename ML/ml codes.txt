practical 1
import pandas as pd 
import matplotlib.pyplot as plt 
df = pd.read_csv("Data.csv").values 
df["Age"].fillna(df["Age"].mean(), inplace=True) 
df["Salary"].fillna(df["Salary"].mean(), inplace=True) 
plt.figure(figsize=(6,4)) 
avg_salary = df.groupby("Country")["Salary"].mean() 
  
avg_salary.plot(kind="bar", color="skyblue", edgecolor="black") 
plt.title("Average Salary by Country") 
plt.ylabel("Average Salary") 
plt.xlabel("Country") 
plt.grid(axis="y", linestyle="--", alpha=0.7) 
plt.show() 
 
plt.figure(figsize=(6,4)) 
colors = {"Yes": "green", "No": "red"} 
for purchase_status in df["Purchased"].unique(): 
    subset = df[df["Purchased"] == purchase_status] 
    plt.scatter(subset["Age"], subset["Salary"],  
                label=purchase_status,  
                color=colors[purchase_status],  
                s=70, alpha=0.7, edgecolors="k") 
plt.title("Age vs Salary (by Purchase Decision)") 
plt.xlabel("Age") 
plt.ylabel("Salary") 
plt.legend(title="Purchased") 
plt.grid(True, linestyle="--", alpha=0.6) 
plt.show() 
 
plt.figure(figsize=(6,4)) 
purchase_counts = df.groupby(["Country", "Purchased"]).size().unstack(fill_value=0) 
purchase_counts.plot(kind="bar", stacked=True, color=["red", "green"], edgecolor="black") 
plt.title("Purchase Decision by Country") 
plt.ylabel("Count") 
plt.xlabel("Country") 
plt.grid(axis="y", linestyle="--", alpha=0.7) 
plt.show()

____________________________________________
practical 2

import pandas as pd 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import LabelEncoder 
from sklearn.naive_bayes import GaussianNB 
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report 
# Dataset 
data = { 
"Country": ["France", "Spain", "Germany", "Spain", "Germany", "France", "Spain", "France", 
"Germany", "France"], 
"Age": [44, 27, 30, 38, 40, 35, None, 48, 50, 37], 
"Salary": [72000, 48000, 54000, 61000, None, 58000, 52000, 79000, 83000, 67000], 
"Purchased": ["No", "Yes", "No", "No", "Yes", "Yes", "No", "Yes", "No", "Yes"] 
} 
df = pd.DataFrame(data) 
# Handle missing values 
df["Age"].fillna(df["Age"].mean(), inplace=True) 
df["Salary"].fillna(df["Salary"].mean(), inplace=True) 
# Encode categorical data 
le_country = LabelEncoder() 
df["Country"] = le_country.fit_transform(df["Country"]) 
le_purchase = LabelEncoder() 
df["Purchased"] = le_purchase.fit_transform(df["Purchased"]) 
# Split features and target 
X = df[["Country", "Age", "Salary"]] 
y = df["Purchased"] 
# Train-test split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) 
# Naive Bayes model 
nb = GaussianNB() 
nb.fit(X_train, y_train) 
y_pred = nb.predict(X_test) 
# Evaluation metrics 
print("Accuracy:", accuracy_score(y_test, y_pred)) 
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred)) 
print("\nClassification Report:\n", classification_report(y_test, y_pred))
____________________________________________
practical 3
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Load dataset
data = load_boston()
X = data.data[:, [5]]  # Using only "RM" feature (avg rooms per house)
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

train_errors = []
test_errors = []
degrees = range(1, 10)

for d in degrees:
    # Polynomial transform
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    # Train model
    model = LinearRegression()
    model.fit(X_train_poly, y_train)

    # Predictions
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)

    # Mean Squared Errors
    train_errors.append(mean_squared_error(y_train, y_train_pred))
    test_errors.append(mean_squared_error(y_test, y_test_pred))

# Plot Bias-Variance Tradeoff
plt.figure(figsize=(7,4))
plt.plot(degrees, train_errors, marker='o', label="Train Error")
plt.plot(degrees, test_errors, marker='s', label="Test Error")
plt.xlabel("Polynomial Degree")
plt.ylabel("MSE")
plt.title("Bias-Variance Tradeoff")
plt.legend()
plt.grid(True)
plt.show()
____________________________________________
practical 4
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Load Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split into train & test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create SVM classifier (RBF kernel)
model = SVC(kernel='rbf', C=1.0, gamma='scale')

# Train model
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
____________________________________________
practical 5
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load California Housing dataset
data = fetch_california_housing()
X = data.data
y = data.target

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("RÂ² Score:", r2)
____________________________________________
practical 6
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load dataset
iris = load_iris()
X = iris.data

# ---------------------------
# 1. Apply K-Means Clustering
# ---------------------------
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# ---------------------------
# 2. Apply PCA (2 Components)
# ---------------------------
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# ---------------------------
# 3. Visualize Clusters
# ---------------------------
plt.figure(figsize=(6, 5))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=60)
plt.title("K-Means Clustering (Visualized with PCA)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.grid(True)
plt.show()
____________________________________________
practical 7
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training & testing data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create Decision Tree classifier
model = DecisionTreeClassifier(criterion="gini", max_depth=None, random_state=42)

# Train model
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
____________________________________________
practical 8
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Bagging with Decision Trees
bagging = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=20,
    random_state=42
)

# Train
bagging.fit(X_train, y_train)

# Predict
y_pred = bagging.predict(X_test)

print("Bagging Accuracy:", accuracy_score(y_test, y_pred))
____________________________________________
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

# AdaBoost with Decision Trees
boost = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)

# Train
boost.fit(X_train, y_train)

# Predict
y_pred2 = boost.predict(X_test)

print("Boosting (AdaBoost) Accuracy:", accuracy_score(y_test, y_pred2))
